{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (413, 600, 10)\n",
      "y shape:  (413,)\n",
      "x_train:  (330, 600, 10)\n",
      "y_train:  (330,)\n",
      "x_test:  (83, 600, 10)\n",
      "y_test:  (83,)\n"
     ]
    }
   ],
   "source": [
    "x = np.load('data/data_lstm/X.npy')\n",
    "y = np.load('data/data_lstm/Y.npy')\n",
    "print(\"x shape: \",x.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,shuffle=True,test_size=0.2)\n",
    "\n",
    "print(\"x_train: \",x_train.shape)\n",
    "print(\"y_train: \",y_train.shape)\n",
    "\n",
    "print(\"x_test: \",x_test.shape)\n",
    "print(\"y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences shape:  torch.Size([330, 600, 10])\n",
      "torch.Size([330])\n",
      "torch.Size([330, 1])\n",
      "pilots size.(0):  330\n",
      "pilots shape:  torch.Size([330, 600, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 50\n",
    "\n",
    "# sequences = torch.FloatTensor([dataset[(seq_len*t):(seq_len*t)+seq_len] for t in range(len(dataset)-seq_len)])\n",
    "pilots = torch.FloatTensor([x_train[t] for t in range(330)])\n",
    "print('sequences shape: ', pilots.shape)\n",
    "# sequences = sequences.cuda()\n",
    "\n",
    "# targets = torch.FloatTensor([targets[t+seq_len-1] for t in range(len(targets)-seq_len)])\n",
    "targets = torch.FloatTensor([y_train[t] for t in range(330)])\n",
    "print(targets.shape)\n",
    "targets = targets.view(-1,1)\n",
    "print(targets.shape)\n",
    "# targets = targets.cuda()\n",
    "\n",
    "print('pilots size.(0): ', pilots.size(0))\n",
    "print('pilots shape: ', pilots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([111, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "def pytorch_rolling_window(x, window_size, step_size):\n",
    "    # unfold dimension to make our rolling window\n",
    "    return x.unfold(0,window_size,step_size)\n",
    "\n",
    "# make a range sequence sample\n",
    "x = pilots[0]\n",
    "\n",
    "# ie. window size of 5, step size of 1\n",
    "\n",
    "print(pytorch_rolling_window(x,50,5).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape : torch.Size([10, 600, 10])\n",
      "x permuted : torch.Size([600, 10, 10])\n",
      "\n",
      "output_0 shape : torch.Size([600, 10, 20])\n",
      "hn shape: torch.Size([1, 10, 20])\n",
      "cn shape: torch.Size([1, 10, 20])\n",
      "Second technique: torch.Size([10, 1])\n",
      "tensor([[ 0.2909],\n",
      "        [    nan],\n",
      "        [-0.1410],\n",
      "        [    nan],\n",
      "        [    nan],\n",
      "        [ 0.2225],\n",
      "        [ 0.2646],\n",
      "        [    nan],\n",
      "        [ 0.2543],\n",
      "        [    nan]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# I think this is a TEST TO CHECK SHAPE/SIZES\n",
    "x = sequences[:10]\n",
    "y = targets[:10]\n",
    "print('x shape :',x.shape)\n",
    "\n",
    "x = x.permute(1, 0, 2)\n",
    "print(\"x permuted :\",x.shape)\n",
    "\n",
    "input_size = 10\n",
    "hidden_size = 40\n",
    "num_layers = 1\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "# lstm = lstm.cuda()\n",
    "\n",
    "output_y, (output_hn, output_cn) = lstm(x)\n",
    "\n",
    "print('\\noutput_0 shape :',output_y.shape)\n",
    "print('hn shape:',output_hn.shape)\n",
    "print('cn shape:',output_cn.shape)\n",
    "\n",
    "\n",
    "# With nn.Linear().\n",
    "LL = nn.Linear(hidden_size, 1)\n",
    "# LL = LL.cuda()\n",
    "output = LL(output_y[-1, :, :])\n",
    "\n",
    "\n",
    "print('Second technique:',output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        \n",
    "        self.input_size = 10\n",
    "        self.hidden_size = 40\n",
    "        self.num_layers = 1\n",
    "        self.dropout = 0.1\n",
    "        self.bias = False\n",
    "        self.batch_first = False\n",
    "        self.bidirectional = False\n",
    "\n",
    "#         self.input_size2 = self.hidden_size\n",
    "#         self.hidden_size2 = 60\n",
    "#         self.num_layers2 = 1\n",
    "#         self.dropout2 = 0.1\n",
    "#         self.bias2 = False\n",
    "#         self.batch_first2 = False\n",
    "#         self.bidirectional2 = False\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size,\n",
    "                            self.hidden_size,\n",
    "                            self.num_layers, \n",
    "                            self.bias,\n",
    "                            self.batch_first,\n",
    "                            self.dropout,\n",
    "                            self.bidirectional)\n",
    "#         self.lstm2 = nn.LSTM(self.input_size2,\n",
    "#                             self.hidden_size2,\n",
    "#                             self.num_layers2, \n",
    "#                             self.bias2,\n",
    "#                             self.batch_first2,\n",
    "#                             self.dropout2,\n",
    "#                             self.bidirectional2)\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size*50*111, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input1, hidden):\n",
    "\n",
    "        out, (h_t, c_t) = self.lstm(input1, hidden)\n",
    "#         out = out[-1][:,None, :]\n",
    "#         out, (h_t2, c_t2) = self.lstm2(out, hidden2)\n",
    "        out = out.reshape(1,222000)\n",
    "#         print(out.shape)\n",
    "        out = self.linear(out)\n",
    "#         out = np.array(out)\n",
    "#         print(\"out.shape: \" ,out.shape)\n",
    "                \n",
    "        return out\n",
    "              \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                 (torch.zeros(self.num_layers, batch_size, self.hidden_size)))\n",
    "        return hidden\n",
    "    \n",
    "#     def init_hidden2(self, batch_size):\n",
    "#         hidden2  = (torch.zeros(self.num_layers2, batch_size, self.hidden_size2),\n",
    "#                  (torch.zeros(self.num_layers2, batch_size, self.hidden_size2)))\n",
    "#         return hidden2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape: torch.Size([600, 10])\n",
      "y_targets.shape: torch.Size([1])\n",
      "Training model for 8 epoch of 1 batches\n",
      "Epoch 1: Loss = 0.77813256\n",
      "Epoch 2: Loss = 3027.42846680\n",
      "Epoch 3: Loss = 5786.68994141\n",
      "Epoch 4: Loss = 6541.01513672\n",
      "Epoch 5: Loss = 6199.41064453\n",
      "Epoch 6: Loss = 5056.06054688\n",
      "Epoch 7: Loss = 3344.66552734\n",
      "Epoch 8: Loss = 1424.08288574\n",
      "x_data.shape: torch.Size([600, 10])\n",
      "y_targets.shape: torch.Size([1])\n",
      "Training model for 8 epoch of 1 batches\n",
      "Epoch 1: Loss = nan\n",
      "Epoch 2: Loss = nan\n",
      "Epoch 3: Loss = nan\n",
      "Epoch 4: Loss = nan\n",
      "Epoch 5: Loss = nan\n",
      "Epoch 6: Loss = nan\n",
      "Epoch 7: Loss = nan\n",
      "Epoch 8: Loss = nan\n",
      "x_data.shape: torch.Size([600, 10])\n",
      "y_targets.shape: torch.Size([1])\n",
      "Training model for 8 epoch of 1 batches\n",
      "Epoch 1: Loss = nan\n",
      "Epoch 2: Loss = nan\n",
      "Epoch 3: Loss = nan\n",
      "Epoch 4: Loss = nan\n",
      "Epoch 5: Loss = nan\n",
      "Epoch 6: Loss = nan\n",
      "Epoch 7: Loss = nan\n",
      "Epoch 8: Loss = nan\n",
      "x_data.shape: torch.Size([600, 10])\n",
      "y_targets.shape: torch.Size([1])\n",
      "Training model for 8 epoch of 1 batches\n",
      "Epoch 1: Loss = nan\n",
      "Epoch 2: Loss = nan\n",
      "Epoch 3: Loss = nan\n",
      "Epoch 4: Loss = nan\n",
      "Epoch 5: Loss = nan\n",
      "Epoch 6: Loss = nan\n",
      "Epoch 7: Loss = nan\n",
      "Epoch 8: Loss = nan\n",
      "x_data.shape: torch.Size([600, 10])\n",
      "y_targets.shape: torch.Size([1])\n",
      "Training model for 8 epoch of 1 batches\n",
      "Epoch 1: Loss = nan\n",
      "Epoch 2: Loss = nan\n",
      "Epoch 3: Loss = nan\n",
      "Epoch 4: Loss = nan\n",
      "Epoch 5: Loss = nan\n",
      "Epoch 6: Loss = nan\n",
      "Epoch 7: Loss = nan\n",
      "Epoch 8: Loss = nan\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_model()\n",
    "learning_rate = 0.001\n",
    "max_grad_norm = 5\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "batch_size = 111\n",
    "num_epochs = 8\n",
    "num_batches = 1\n",
    "\n",
    "#for every pilot, I train 5 epochs of 1 batch\n",
    "for a_pilot in range(5):\n",
    "    \n",
    "    #600 datapoints per pilot\n",
    "    x_data = pilots[a_pilot]\n",
    "    \n",
    "    y_targets = targets[a_pilot]\n",
    "    print(\"x_data.shape:\",x_data.shape)\n",
    "    print(\"y_targets.shape:\", y_targets.shape)\n",
    "    \n",
    "    sequences = pytorch_rolling_window(x_data,50,5)\n",
    "#     print(sequences.shape)\n",
    "    \n",
    "    sequences = np.transpose(sequences,(2,0,1))\n",
    "#     print(sequences.shape)\n",
    "    \n",
    "    num_sequences = 111\n",
    "    num_batches = num_sequences // batch_size\n",
    "    \n",
    "    print(\"Training model for {} epoch of {} batches\".format(num_epochs, num_batches))\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        #reset the gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        #initialize the hidden state\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        #complete a forward pass\n",
    "        y_pred = model(sequences, hidden)\n",
    "\n",
    "        #calculate the loss with the loss_fn\n",
    "        loss = loss_fn(y_pred, y_targets)\n",
    "#         print(y_pred.shape, y_targets.shape, sequences.shape)\n",
    "\n",
    "        #compute the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        #clip to the gradient to avoid exploding gradient\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        #make on step with optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        #accumulate the total loss\n",
    "        total_loss += loss.data\n",
    "\n",
    "        print(\"Epoch {}: Loss = {:.8f}\".format(epoch+1, total_loss/num_batches))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 1 epoch of 1 batches\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-d9ffee88eed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# Get input and target batches and reshape for LSTM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# Reset the gradient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-d9ffee88eed2>\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(x, y, i, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0my_pilot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0my_pilot_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpilot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_pilot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pilot_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#     batch_y = np.array(batch_y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def get_batch(x, y, i, batch_size):\n",
    "    pilot = 0\n",
    "    \"\"\"Generate batch data for x and y.\"\"\"\n",
    "#     print(x.shape, y.shape)\n",
    "#     if x.dim() == 2: \n",
    "#         x = x.unsqueeze(2)\n",
    "    batch_x = x_train[pilot,(i*batch_size):(i*batch_size)+batch_size, :]\n",
    "#     batch_y = y_train[pilot,(i*batch_size):(i*batch_size)+batch_size]\n",
    "    y_pilot = np.ones(600)\n",
    "    y_pilot_train = (y_train[pilot]*j for j in y_pilot)\n",
    "    batch_y = y_pilot_train[(i*batch_size):(i*batch_size)+batch_size]\n",
    "#     batch_y = np.array(batch_y)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Reshape Tensors into (seq_len, batch_size, input_size) format for the LSTM.\n",
    "#     batch_x = batch_x.transpose(0, 1)\n",
    "#     batch_y = batch_y.transpose(0, 1)\n",
    "    \n",
    "    return batch_x, batch_y\n",
    "\n",
    "# Build model.\n",
    "input_size = 10\n",
    "hidden_size = 40\n",
    "num_layers = 1\n",
    "model = LSTM_model()\n",
    "# model = model.cuda()\n",
    "\n",
    "# Optimizer and loss function.\n",
    "learning_rate = 0.001 #0.02\n",
    "max_grad_norm = 5\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train model.\n",
    "batch_size = 600\n",
    "# batch_size2 = 1\n",
    "num_epochs = 1\n",
    "\n",
    "sequences = pilots[0]\n",
    "\n",
    "num_sequences = sequences.size(0)\n",
    "# print(\"seq:\",num_sequences)\n",
    "num_batches = num_sequences // batch_size\n",
    "\n",
    "print(\"Training model for {} epoch of {} batches\".format(num_epochs, num_batches))\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    # Shuffle input and target sequences.\n",
    "#     idx = torch.randperm(sequences.size(0))\n",
    "#     x = sequences[idx]\n",
    "#     y = targets[idx]\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        # Get input and target batches and reshape for LSTM.\n",
    "        batch_x, batch_y = get_batch(sequences, targets, i, batch_size)\n",
    "\n",
    "        # Reset the gradient.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Initialize the hidden states (see the function lstm.init_hidden(batch_size)).\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        hidden2 = model.init_hidden2(batch_size2)\n",
    "        \n",
    "        # Complete a forward pass.\n",
    "        y_pred = model(batch_x, hidden, hidden2)\n",
    "        \n",
    "        # Calculate the loss with the 'loss_fn'.\n",
    "        loss = loss_fn(y_pred, batch_y)\n",
    "        print(y_pred.shape, batch_y.shape, batch_x.shape)\n",
    "        \n",
    "        # Compute the gradient.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip to the gradient to avoid exploding gradient.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        # Make one step with optimizer.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the total loss.\n",
    "        total_loss += loss.data\n",
    "\n",
    "    if ((epoch+1)%10) == 0:\n",
    "      print(\"Epoch {}: Loss = {:.8f}\".format(epoch+1, total_loss/num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for test!!\n",
    "\n",
    "#TEST THE SIZES / SHAPES\n",
    "\n",
    "def get_batch(x, y, i, batch_size):\n",
    "    \"\"\"Generate batch data for x and y.\"\"\"\n",
    "#     print(x.shape, y.shape)\n",
    "    if x.dim() == 2:\n",
    "        x = x.unsqueeze(2)\n",
    "    batch_x = x[(i*batch_size):(i*batch_size)+batch_size, :]\n",
    "    batch_y = y[(i*batch_size):(i*batch_size)+batch_size]\n",
    "\n",
    "    # Reshape Tensors into (seq_len, batch_size, input_size) format for the LSTM.\n",
    "    batch_x = batch_x.transpose(0, 1)\n",
    "    batch_y = batch_y.transpose(0, 1)\n",
    "    \n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "\n",
    "test = generate_schedules(1, 40, 1,4,2,1)\n",
    "#print(aDataset)\n",
    "testtargets = test[1]\n",
    "# print(len(testtargets))\n",
    "testtargets = testtargets[1:]\n",
    "testdata = test[0]\n",
    "\n",
    "\n",
    "print(len(testdata))\n",
    "print(len(testtargets))\n",
    "\n",
    "testdata = np.asarray(testdata)\n",
    "testtargets = np.asarray(testtargets)\n",
    "testtargets = testtargets[:,None]\n",
    "print(testdata.shape)\n",
    "\n",
    "\n",
    "testdata = torch.FloatTensor([testdata[t] for t in range(4)])\n",
    "print('testdata shape: ', testdata.shape)\n",
    "testdata = testdata.cuda()\n",
    "\n",
    "\n",
    "testtargets = torch.FloatTensor([testtargets[t] for t in range(4)])\n",
    "print('testtargets shape: ', testtargets.shape)\n",
    "testtargets = testtargets.cuda()\n",
    "\n",
    "i=0\n",
    "batch_x, batch_y = get_batch(testdata, testtargets, i, batch_size=4)\n",
    "# print(batch_x.shape)\n",
    "# print(batch_y.shape)\n",
    "\n",
    "\n",
    "hidden = model.init_hidden(4)\n",
    "hidden2 = model.init_hidden2(1)\n",
    "\n",
    "# out = model(batch_x, hidden, hidden2)\n",
    "# out.shape\n",
    "# print(out.shape)\n",
    "\n",
    "\n",
    "testtargets.size(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST THE MODEL\n",
    "\n",
    "\n",
    "def predict_one_step(model, batch_x, input_size, num_steps):\n",
    "    \"\"\"Predicts one-step ahead for each sequence of the test set.\"\"\"\n",
    "    hidden = model.init_hidden(4)\n",
    "    hidden2 = model.init_hidden2(1)\n",
    "    y_pred = model(batch_x, hidden, hidden2)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "  \n",
    "\n",
    "one_step_predictions = predict_one_step(model, batch_x, 6, testtargets.size(0))\n",
    "print(one_step_predictions)\n",
    "print('test y :' ,testtargets.data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
